<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style>
  </style>
  <script src="./tf.min.js"></script>
  <script type="module" src="./demo.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">{
      "title": "Self-Classifying MNIST",
      "description": "Training an end-to-end differentiable, self-organising cellular automata for classifying MNIST digits.",
      "authors": [
        {
          "author": "Ettore Randazzo",
          "authorURL": "https://oteret.github.io/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Alexander Mordvintsev",
          "authorURL": "https://znah.net/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Eyvind Niklasson",
          "authorURL": "https://eyvind.me/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Michael Levin",
          "authorURL": "http://www.drmichaellevin.org",
          "affiliation": "Allen Discovery Center at Tufts University",
          "affiliationURL": "http://allencenter.tufts.edu"
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <style>
    figure {
      text-align: center;
      margin-bottom: 0.5em;
      margin-top: 0.5em;
    }
    figure img {
      max-width: 100%;
      width: unset;
    }
    video {
      max-width: 100%;
    }
    .colab-root {
      display: inline-block;
      background: rgba(255, 255, 255, 0.75);
      padding: 4px 8px;
      border-radius: 4px;
      font-size: 11px!important;
      text-decoration: none;
      color: #aaa;
      border: none;
      font-weight: 300;
      border: solid 1px rgba(0, 0, 0, 0.08);
      border-bottom-color: rgba(0, 0, 0, 0.15);
      text-transform: uppercase;
      line-height: 16px;
    }

   span.colab-span {
      background-image: url(images/colab.svg);
      background-repeat: no-repeat;
      background-size: 20px;
      background-position-y: 2px;
      display: inline-block;
      padding-left: 24px;
      border-radius: 4px;
      text-decoration: none;
    }

    a.colab-root:hover{
      color: #666;
      background: white;
      border-color: rgba(0, 0, 0, 0.2);
    }

    /* TOC */
    @media(max-width: 1000px){
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    } 
    
    @media (min-width: 1000px){
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1180px){
      d-contents {
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }
    
    d-contents nav ul li {
      margin-bottom: .25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }


    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
        text-decoration: none;
    }

    /* code blocks to margins */
    @media (min-width: 1600px) {
      d-code {
        margin-top: -10px;
        grid-column-start: 12;
        grid-column-end: 14; 
      }
    }
    /* so title is on one line */
    d-title h1, d-title p {
      grid-column: middle;
    }
    
    d-article table th, d-article table td {
      text-align: center;
    }

    @media(max-width: 499px){
      d-article table th, d-article table td {
        font-size: 9px;
        padding: 0px 2px;
      }
    }

  </style>
  <script>
  // hack to edit font size in code snippets. guaranteed a better way to do 
  // this, but I'm not a webdev
  window.onload = function() {
    setTimeout(() => { document.querySelectorAll("d-code").forEach(function(e) {e.shadowRoot.querySelector('#code-container').style.fontSize = "0.7em"}); }, 3000);
  }
  </script>
  <d-title>
    <h1>Self-Classifying MNIST digits</h1>
    <p>Neural Cellular Automata Model of Distributed Coordination</p>

  
<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
    <symbol id="playIcon" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="pauseIcon" viewBox="0 0 24 24"><path d="M6 19h4V5H6v14zm8-14v14h4V5h-4z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="resetIcon" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"></path></symbol>
    <symbol id="colorwheel" viewBox="0 0 24 24"> 
  <path
     d="m 511.48146,205.29524 c 11.90567,44.43259 11.90567,84.97693 0,129.40952 L 487.31476,349.3424 270,270 490.20449,192.09986 z"
     id="path16"
     style="fill:#fefe33" />
  <path
     d="m 334.70476,28.518543 c 44.43259,11.905676 79.54502,32.17785 112.07193,64.704761 L 447.40047,118.09589 270,270 313.06484,40.508134 z"
     id="path20"
     style="fill:#fb9902" />
  <path
     d="m 446.77669,93.223304 c 32.52692,32.526916 52.79909,67.639346 64.70477,112.071936 L 270,270 z"
     id="path18"
     style="fill:#fabc02" />
  <path
     d="M 93.223305,93.223305 C 125.75022,60.696393 160.86265,40.42422 205.29524,28.518543 L 231.20546,44.501656 270,270 92.739568,120.0571 z"
     id="path28"
     style="fill:#fe2712" />
  <path
     d="m 205.29524,28.518543 c 44.43259,-11.905676 84.97693,-11.905676 129.40952,0 L 270,270 z"
     id="path22"
     style="fill:#fd5308" />
  <path
     d="m 28.518543,334.70476 c -11.905676,-44.43259 -11.905676,-84.97693 0,-129.40952 L 56.311276,186.62718 270,270 55.854788,349.40527 z"
     id="path26"
     style="fill:#8601af" />
  <path
     d="M 28.518543,205.29524 C 40.424219,160.86265 60.696393,125.75022 93.223305,93.223305 L 270,270 z"
     id="path30"
     style="fill:#a7194b" />
  <path
     d="M 205.29524,511.48146 C 160.86265,499.57578 125.75022,479.30361 93.223305,446.7767 L 95.307837,418.58874 270,270 231.0453,499.70648 z"
     id="path8"
     style="fill:#0247fe" />
  <path
     d="M 93.223305,446.7767 C 60.696393,414.24978 40.42422,379.13735 28.518543,334.70476 L 270,270 z"
     id="path24"
     style="fill:#3d01a4" />
  <path
     d="m 446.7767,446.7767 c -32.52692,32.52691 -67.63935,52.79908 -112.07194,64.70476 L 310.45335,496.38826 270,270 446.04632,421.15701 z"
     id="path12"
     style="fill:#66b032" />
  <path
     d="m 334.70476,511.48146 c -44.43259,11.90567 -84.97693,11.90567 -129.40952,0 L 270,270 z"
     id="path10"
     style="fill:#0391ce" />
  <path
     d="M 511.48146,334.70476 C 499.57578,379.13735 479.30361,414.24978 446.7767,446.7767 L 270,270 511.48146,334.70476 z"
     id="path14"
     style="fill:#d0ea2b" />
  <circle
     cx="270"
     cy="270"
     r="153.79581"
     id="circle32"
     style="fill:#ffffff" />
  </symbol> 
</svg>



<style>
#demo {
    font-size: 14px;
    user-select: none;
    grid-template-columns: auto;
    grid-template-rows: auto auto auto;
    grid-auto-flow: column;
    row-gap: 10px;
}

.hint a {
  color: inherit;
}

@media (min-width: 1000px) {
  #demo {
    grid-template-columns: 1fr 300px;
    grid-template-rows: auto auto;
  }
  #demo-controls {
    grid-row: 1/3;
  }
}

#demo-canvas {
    border: 1px solid lightgrey;
    image-rendering: pixelated;
    touch-action: none;
    width: 100%;
}

#demo-controls {
    line-height: 1em;
    display: grid;
    grid-template-columns: 120px auto;
    grid-template-rows: auto 60px 80px 75px 1fr;
    row-gap: 20px;
    overflow: hidden;
}

@media (min-width: 1000px){
  #demo-controls {
    grid-template-rows: auto 60px 70px 70px 1fr 1fr;
  }
}

#pattern-selector {
    grid-column: 1/3;
    display: grid;
    grid-template-columns: repeat(5, auto);
    justify-items: center;
}
@media (max-width: 1000px) and  (min-width: 500px) {
  #pattern-selector {
    grid-template-columns: repeat(10, auto);
  }
}

#pattern-selector * {
    width: 100%;
    /* background-image: url('images/emoji.png'); */
    cursor: pointer;
}
.icon {
    width: 30px; height: 30px;
    background: steelblue;
    fill: white;
    border-radius: 20px;
    padding: 5px;
    margin: 2px;
    cursor: pointer;
}
#model-selector {
    line-height: 1.4em;
}
#demo-tip{
    display: grid;
    grid-template-columns: 40px auto;
    align-items: center;
    column-gap: 10px;
    margin-bottom: 20px;
}
#pointer {
    width: 40px;
}
#status {
    font-size: 12px;
    color: rgba(0, 0, 0, 0.6);
    font-family: monospace;
}
#model-hints {
    color: rgba(0, 0, 0, 0.6);
    grid-column: 1/3;
}
#model-hints span {
    display: none;
}
.hint {
    color: rgba(0, 0, 0, 0.6);
    line-height: 1.4em;
    user-select: text;
}

input[type=range] {
  -webkit-appearance: none; /* Hides the slider so that custom slider can be made */
  width: 95%; /* Specific width is required for Firefox. */
  background: transparent; /* Otherwise white in Chrome */
  margin-bottom: 8px;
}

.hint a {
  font-size: 90%;
}

@media (max-width: 350px) {
  .hint a {
    font-size: 75%;
  }
}

input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
}

input[type=range]:focus {
  outline: none; /* Removes the blue border. You should probably do some kind of focus styling for accessibility reasons though. */
}

input[type=range]::-ms-track {
  width: 100%;
  cursor: pointer;

  /* Hides the slider so custom styles can be added */
  background: transparent;
  border-color: transparent;
  color: transparent;
}

/* Thumb */

/* Special styling for WebKit/Blink */
input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
  height: 16px;
  width: 16px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  margin-top: -7px; /* You need to specify a margin in Chrome, but in Firefox and IE it is automatic */
}

/* All the same stuff for Firefox */
input[type=range]::-moz-range-thumb {
  height: 16px;
  width: 16px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  border: none;
}

/* All the same stuff for IE */
input[type=range]::-ms-thumb {
  height: 16px;
  width: 16px;
  border-radius: 50%;
  background: grey;
  cursor: pointer;
}

/* Track */

input[type=range]::-webkit-slider-runnable-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]:focus::-webkit-slider-runnable-track {
  background: rgba(0, 0, 0, 0.15);
}

input[type=range]::-moz-range-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}
input[type=range]::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}

input[type="radio"] {
    background-color: steelblue;
}

#colab-hero-div { 
  grid-column: 1/3;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-top-width: 1px;
  border-top-style: solid;
  border-top-color: rgba(0, 0, 0, 0.1);
  padding-top: 15px;
}

#colab-hero {
  margin: auto;
  display: block;
  text-align: center;
  width: 200px;
  height: 16px;
}

#eraser {
  height: 50px;
  width: 50px;
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0), rgb(255, 255, 255)), url(eraser.png);
  display: inline-block;
  background-repeat: no-repeat;
  background-size: contain;
}

#pencil {
  height: 50px;
  width: 50px;
  background-image: linear-gradient(to left, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0), rgb(255, 255, 255)), url(pencil.png);
  display: inline-block;
  background-repeat: no-repeat;
  background-size: contain;
}

#paletteHint {
  grid-column: 1/3;
  color: rgba(0, 0, 0, 0.6);
  line-height: 1.4em;
  user-select: text;
  font-size: 75%;
}

</style>

<div class="l-body-outset grid" id="demo">
  
    <!-- fake canvas dimensions to ensure square in css dimensions -->  
    <canvas id="demo-canvas" width="512" height="512" class="color_heavy"></canvas>

    <div id="demo-tip">
            <img id="pointer" src="images/pointer.svg">
            <div class="hint">
                Click or tap and drag to draw shapes in the canvas. 
                Select different digits to load digits from the training set. Click again to get a different sample of the same digit.       
            </div>
    </div>

    <div id="demo-controls">
        <div id="pattern-selector" class="color_heavy">

        </div>
        <div style="text-align: center">
            <span id="play-pause">
                <svg class="icon" id="play"><use xlink:href="#playIcon"></use></svg>
                <svg class="icon" id="pause" style="display: none;"><use xlink:href="#pauseIcon"></use></svg>
            </span>
            <svg class="icon" id="reset"><use xlink:href="#resetIcon"></use></svg>
        </div>
        <div>
            Speed: <span id="speedLabel"></span><br>
            <input type="range" id="speed" min="-3" max="3" step="1" value="-2"><br>
            <div id="status">
               <!--  Step <span id="stepCount"></span> -->
                (<span id="ips"></span> step/s)
            </div>
        </div>
        <div id="eraser-pencil" style="text-align: center">
          <div id="eraser" style="filter: grayscale();"></div>
          <div id="pencil" ></div>
        </div>
        <div class="slidecontainer">
          Brush size:
          <input type="range" min="5" max="10" value="4" class="slider" id="brushSlider">
          <div id="status">(<span id='radius'>2.5</span> px)</div>
        </div>
        <div id="colorwheelicon" style="text-align: center">
          <img id="pointer" src="images/colorwheel.svg">
        </div>
        <div class="slidecontainer">
          Palette:
          <input type="range" min="0" max="360" value="0" class="slider" id="hueSlider">
          <div id="status">(<span id='hueValue'>0</span> deg)</div>
        </div>
        <div id="paletteHint">This article relies on using color to demonstrate classification label. If you have trouble distinguishing the colours of digits in the above legend, please try and adjust the slider above to see if there is an alternative colour palette for you. The chosen palette will propagate throughout the article.</div>
        <div id="colab-hero-div">
          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/mnist_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a>
        </div>
    </div>
</div>

<script type="module">
    import { mnistDemo } from './demo.js'
    mnistDemo("demo", "demo-canvas");
</script>

</d-title>

<d-byline></d-byline>


<d-article>
<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#model">Model</a></div>
    <div><a href="#experiment-1">Experiments</a></div>
    <ul>
      <li><a href="#experiment-1">Self-classify, persist & mutate</a></li>
      <li><a href="#experiment-2">Solving the flickering regression</a></li>
    </ul>
    <div><a href="#observed-robustness">Observed robustness and limitations</a></div>
    <div><a href="#related-work">Related Work</a></div>
    <div><a href="#discussion">Discussion</a></div>
  </nav>
</d-contents>
<p>The Growing Neural Cellular Automata article <d-cite key="mordvintsev2020growing"></d-cite> demonstrated how a simple cellular automata (CAs) can self-organise  to generate complex shapes, all the while being resistant to perturbations. The models parameterizing the cells’ rules are parameter-efficient and end-to-end differentiable. In this work we show how a CA can be repurposed for a common task in machine learning - classification. We pose the question: <strong>can CAs</strong>, with living cells arranged in the shape of a certain digit,<strong> </strong><i>come to total agreement regarding what digit they compose?</i></p>

<p>Suppose there are agents arranged in a grid. They do not know what position of the grid they occupy, nor can they see or communicate any further than their immediate neighbors. They can also observe whether a neighbor is missing. Now suppose that these agents are arranged to form a large digit, as seen from above with a certain angle. Given that all the agents operate under the same rules, can they form a communication protocol such that after a number of iterations of communication <i>all of the agents know which digit they are forming?</i></p>
<p>Furthermore, if some agents were to be removed and added to form a new digit from a preexisting one, would they be able to know which the new digit is?</p>
<p>Because digits are not rotationally invariant (i.e. 6 is a rotation of 9), we assume the agents must be made aware of their orientation with respect to the grid. Therefore, while they do not know where they are, they know where up, down, left and right are.</p>


<p>Introducing the Self-classifying MNIST digit task:</p>

<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:500px;">
  <object data="figures/mnist_digits.png" type="image/png" style="width:100%"></object>
</figure></p>

<p>Each sample of the MNIST dataset <d-cite key="lecun_mnist"></d-cite> consists of a 28x28 image with a single greyscale channel. The label is an integer in [0,9].</p>
<p>The goal is for each cell residing in the digit to correctly output the label of the digit. We therefore make a distinction between alive and dead cells, by rescaling the grey channel to [0, 1] and treating a cell as alive if and only if their greyscale channel value is $> 0.1$. Each digit label is paired with a random color which is used for visualizing the current classification of each alive cell. Some readers may have difficulties with distinguishing all of the label colors; in this case, we recommend choosing a suitable color palette from the demo section on top of the article.</p>
<h2 id='model'>Model</h2>

<p>The mode used in this article is a variation of the one used in Growing Cellular Automata article <d-cite key="mordvintsev2020growing"></d-cite>. For readers unfamiliar with its implementation, we refer to the model section in the work.</p>

<p>The variations to the Growing CA model we apply hereby follow:</p>
<h3>Target classes</h3>
<p>The work in Growing CA had a label in form of an RGB image, with the loss applied to encourage the first three state channels of the cells to approximate this image. For our experiments, we treat the last ten channels of our cells as distribution over the likelihood of being a given digit. During inference, we perform an argmax over these channels to extract the label for each alive cell.</p>
<h3>Aliveness and cell states</h3>
<p>The concept of a cell being alive or dead in Growing CA was a function of its own alpha channel, as well as its neighbours. For our experiments, the distribution of alive and dead cells is explicitly encoded in the input data greyscale channel, and we perform computations for alive cells only, whose respective greyscale channel is $> 0.1$. While the greyscale channel is immutable, cells are made aware of the greyscale values for themselves and their neighborhood. Therefore, given 19 mutable cell states (9 general purpose states, 10 states for digit classification) and a fixed greyscale state, each cell perceives 19 + 1 states from the neighborhood, but outputs state changes for only 19 mutable states. </p>

<h3>Perception</h3>
<p>The Growing CA model made use of a fixed 3x3 convolution to extract the identity state and an estimate of the gradient of state in x and y coordinates (approximated by Sobel filters). We found that fully trainable 3x3 kernels outperform their fixed counterpart, and use trainable kernels in this work.</p>

<p>Overall, the model remains small by standards of contemporary deep learning (<25k parameters). In this work, we are interested in demonstrating a novel approach to classification and do not attempt to maximise the validation accuracy of the model by increasing the number of parameters or other tuning, although we suspect this would be possible.</p>
<h2 id='experiment-1'>Experiment 1: Self-classify, persist and mutate</h2>
<p>This first experiment makes use of the results of the experiments discussed in the Growing CA article. We therefore train with a pool of initial samples, to allow the model to learn to persist, and perturb the converged states. The kind of perturbation performed in this work is, however, different. In the Experiment 3 of the Growing CA article, the authors erased random crops of the states to make the CAs resistant to destructive perturbations. In this work, our goal is of broader scope: while having regenerative properties is favourable, we also want our CAs to <i>learn to understand when the underlying digits change</i>.</p>

<p>We accomplish that by applying random digit mutations: from an original digit and state, we sample a new digit and erase all cell states that are not present in both digits. This kind of mutation inherently teaches CAs to learn to regenerate. More importantly, it exposes CAs to training scenarios where all of the remaining alive cells are 90% of the time misclassifying the new digit. This in turn forces our CAs to learn to change their own classifications under certain circumstances.</p>

<p>We use a pixel-wise (cell-wise) cross entropy loss  on the last ten channels of each pixel, applying it after letting the CA evolve for 20 steps.</p>
<p><figure>
    <video loop autoplay playsinline muted width="640px">
      <source src="figures/ce_runs.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure></p>

<p>The video above shows the CA classifying a batch of digits for 200 steps, which are then mutated, and we let the system evolve and classify for a further 200 more steps.</p>

<p>The results look promising overall, and we can see how our CA are able to recover from mutations. However, keen readers may notice there is often a lack of total agreement between the cells. Often, the majority of the digit is correctly classified, but some straggler cells here and there are still convinced they are part of a different digit, often switching back and forth in an oscillating pattern, creating a flickering effect in the visualization. Since we aim at obtaining both correct and stable configurations, we want to encourage total agreement. The next experiment will try to do so.</p>
<h2 id='experiment-2'>Experiment 2: Solving the flickery regression</h2>
<p>A good starting point is to try and quantify the aforementioned behaviour. One metric we could track is <strong>average cell accuracy</strong>: the average percentage of cells who correctly classify the digits in a batch over time, before and after the digit being mutated:</p>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width: 700px">
  <object data="figures/ce_accuracy.png" type="image/png" style="width:100%"></object>
</figure></p>
<p>In the figure above, we show the average percentage of correctly classified cells across the entire test set, over 400 CA steps. After 200 steps, we randomly mutate the digit to a new one (causing the brief drop to 10% accuracy, as the cells re-organise).</p>

<p>The above graph shows how the average cell accuracy starts from 10% to then quickly reach  96% at 80 steps and slowly decay by 1% over 100 steps. After the mutations occurring in step 200, the average cell accuracy restarts from 10% and does not appear to behave significantly differently from a fresh start. The slow decay is related to the lack of total agreement, but doesn’t capture it sufficiently well. We then introduce a new metric that tracks this behavior more accurately: <strong>total agreement</strong>. We define total agreement to be the proportion of digits in a training batch where all cells agree with each other on what digit they think they are.</p>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/ce_agreement.png" type="image/png" style="width:100%"></object>
</figure></p>

<p>In the figure above we see how total agreement starts at 0%, and then spikes up to roughly 78%, only to lose more than 10% agreement during the next 100 steps. Again, there does not appear to be a significantly different behaviour after mutation. This graph pinpoints an interesting phenomenon: the total agreement decreases significantly with time. This CA clearly is not stable at classifying digits in the long term - as time goes on, cells get less sure of themselves. Let us inspect the inner states of the CA to see if we can find some insight.</p>

<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/ce_magnitude.png" type="image/png" style="width:100%"></object>
</figure></p>
<p>The figure above shows the evolution over time of the average magnitude of each mutable state of active cells (solid line), and the average magnitude of the residual updates for the active cells firing at each timestamp (dotted line). We can observe two properties of this CA: the average magnitude of each cell’s internal states is unbounded, and the average magnitude of the residual updates does not change significantly over time. A desirable CA would likely stabilize its magnitude and learn to stop updating itself, once cells find an agreement.</p>


<p>Let us try to address the aforementioned problems in two ways:</p>
<p><strong>Change the loss to L2: </strong>Cross-entropy loss inherently encourages the correct class to continually be pushed indefinitely higher than other predicted classes. Having high magnitude in the classification channels may in turn lead the remaining channels to adapt to a high magnitude regime. This, combined with a lack of a shared reference range for the target labels may make it difficult for the digits to stabilize. We instead investigate changing the target to be a 28x28x10 dimensional tensor, where each channel is 1 for the target class, and 0 otherwise, and using a pixelwise L2 loss. Intuitively, the solution should be more stable since not only the outputs are now encouraged to stay close to the range $[0, 1]$, but a properly classified digit in a cell will have exactly one output set to 1, and the rest to 0. This should decrease the magnitude of all internal states and give an absolute reference range for the target labels. Absolute reference ranges can in turn aid with stabilising the internal states, once an agreement is reached.</p>
<p><strong>Add noise to the residual</strong>: A key idea in most schemes of regularization is to add noise to make a classifier or model more robust.  We add noise sampled from a normal distribution with 0 mean and 0.02 standard-deviation to the residual. The noise is added before the random mask for asynchronous updates.</p>
<p><figure>
    <video loop autoplay playsinline muted width="640px">
      <source src="figures/l2_runs.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure></p>
<p>The video above shows a batch of runs with the above augmentations in place. Qualitatively, the result looks much better - with less flickering and more total agreement. We now compare it to the original method:</p>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/ce_vs_l2_metrics.png" type="image/png" style="width:100%"></object>
</figure></p>

<div id=accTable style=overflow-x:scroll;grid-column:page;max-width:700px;margin-left:auto;margin-right:auto;><table class=model_table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Top accuracy</th>
<th align="center">Accuracy @ 200 </th>
<th align="center">Top agreement</th>
<th align="center">Agreement @ 200</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">CE</td>
<td align="center"><b>96.2 @ 80</b></td>
<td align="center"><b>95.3</b></td>
<td align="center">77.9 @ 80</td>
<td align="center">66.2</td>
</tr>
<tr>
<td align="center">L2</td>
<td align="center">95.0 @ 95</td>
<td align="center">94.7</td>
<td align="center">85.5 @ 175</td>
<td align="center">85.2</td>
</tr>
<tr>
<td align="center">L2 + Noise</td>
<td align="center">95.4 @ 65</td>
<td align="center"><b>95.3</b></td>
<td align="center"><b>88.2 @ 190</b></td>
<td align="center"><b>88.1</b></td>
</tr>
</tbody>
</table>
</div>


<p>The figure and table above show that CE achieves the highest accuracy of all models at roughly 80 steps. However, the accuracy at 200 steps is the same as the L2 + Noise model. Even though either accuracy and agreement degrade over time for all models, it is evident that the L2 + Noise is the most stable of them all. Moreover, the total agreement after 200 steps of L2 + Noise is 88%, more than 20% higher than the CE counterpart. One can extrapolate this degrading behavior observed continues after 200 steps, exacerbating the difference between CE and L2 models.</p>
<h3>Internal states</h3>
<p>Below we compare the internal states of the newly trained models and the one trained on Experiment 1:</p>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/magnitude_comparison.png" type="image/png" style="width:100%"></object>
</figure></p>
<p>The figure above shows how switching to an L2 loss successfully stabilizes the magnitude of the states, and how residual updates quickly converge to very small values after an agreement is about to be reached.</p>

<p><figure>
    <video loop autoplay playsinline muted width:100%>
      <source src="figures/l2n_horiz_states.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure></p>
<p>Here we show the resulting dynamics of the internal states for the final model. For visualization purposes, the internal state values are squashed with an arctan function. The states converge to stable configurations very quickly and the state channels are generally continuous to their neighbors, resulting in spatial gradients. Applying digit mutations causes the CA to readapt steadily to the new shape and classification.</p>
<h2 id='observed-robustness'>Observed robustness and limitations</h2>
<h3>Adaptation to mutation</h3>
<p>The usage of random mutations during training was chosen to train a CA that would be responsive to changes. This is of critical importance while drawing, in order to allow live updates as different digits take shape (an eight being completed from a six, for instance). We encourage the readers to play with the interactive demo, and experience this for themselves. In this section, we want to showcase a few behaviours we found interesting.</p>
<p><figure>
    <div>
<video loop autoplay playsinline muted width="320px">
      <source src="figures/drawing_mutations.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    </div>
   <div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
</figure></p>

<p>The video above shows how the CA is able to interactively adjust to our own writing, and to change classification based on changes on its shape.</p>
<h3>Robustness to out-of-training configurations</h3>
<p>In the field of Machine Learning, it is of particular interest to see how trained models behave with out-of-training data. In the experiments sections of this article, we have already evaluated our model on the test set to make sure it does not overfit to the train set. In this section, we go further and see how much we can vary shapes of digits until the model is no longer capable of classifying digits correctly. Each type of model contains sets of inductive biases that render them more or less robust to certain types of out-of-training situations. Our model can be implemented by convolutional layers and we therefore expect them to share some key properties, such as translation invariance. Moreover, the local and self-organising nature of this model may showcase other interesting properties.</p>
<p><figure>
<div>
    <video loop autoplay playsinline muted width="320px">
      <source src="figures/drawing_bad.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
   <div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
</figure></p>

<p>Above, we can see our CA failing at classifying properly some variants of 1 and 9. This is likely due to MNIST training data not being sufficiently representative of all writing styles. We hypothesize that more varied and extensive datasets would help diminishing this problem. Interestingly, the CAs often converge to oscillating attractors in these situations. This property could not arise from static classifiers such as traditional convolutional neural networks.</p>

<p><figure>
<div>
    <video loop autoplay playsinline muted width="320px">
      <source src="figures/mnist_ablation.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
   <div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
</figure></p>
<p>As hinted before, the resulting CA is translation invariant. Above we show how the model is also scaling invariant for out-of-training scaling of digits up to a certain point. Alas, it does not generalize well enough to work for arbitrary lengths and widths.</p>
<h2 id='related-work'>Related work</h2>
<p>This article is follow-up work of the Growing Neural Cellular Automata article in this thread <d-cite key="mordvintsev2020growing"></d-cite>, and it is meant to be read after the latter. In this article, we avoid repeating work related to both articles, and we refer to the Growing Neural Cellular Automata article for the model and biological backgrounds.</p>

<p><strong>MNIST and CA</strong></p>
<p>Since CAs are easily applied to two dimensional grids, many researchers wondered if they could use them somehow to classify the MNIST dataset <d-cite key="lecun_mnist"></d-cite>. Work we aware of combines CAs with Reservoir Computing <d-cite key="alej2018reservoir, alej2020reservoir"></d-cite>, Boltzmann Machines <d-cite key="matsubara2018"></d-cite>, Evolutionary Strategies <d-cite key="oliveira2008"></d-cite>, and ensemble methods <d-cite key="WALI201877, jastrzebska2017"></d-cite>. To the best of our knowledge, we are the first to train end-to-end differentiable Neural CAs for classification purposes, and we are the first to introduce the Self-classifying variant of MNIST, where each pixel in the digit needs to coordinate locally in order to understand their label.</p>
<h2 id='discussion'>Discussion</h2>
<h3>Embryogenetic Modeling</h3>
<p><figure>
<div>
    <video loop autoplay playsinline muted width:100%>
      <source src="figures/mnist_chimeras.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
 </div>
 <div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:100%"></object>
   </div>
</figure></p>
<h3>Engineering and Machine learning</h3>
<p>This article serves as a proof of concept for how a simple self-organising system such as CA can be used for a task such as classification if trained end-to-end through backpropagation.</p>

<p>The resulting model adapts to writing and erasing, and is surprisingly robust to certain ranges of digit stretching and brush widths. We hypothesize that concise self-organising models may be inherently robust to generalisations, and we encourage future work to test this hypothesis.</p>
</d-article>
<d-appendix>
<h3>Acknowledgments</h3>
<p>We thank Zhitao Gong, Sam Greydanus, Alex Groznykh, Nick Moran, Peter Whidden for their valuable conversations and feedback.</p>
<h3>Author Contributions</h3>
<p><strong>Research:</strong> Alexander came up with the Self-Organising Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Alexander came up with the self-classifying MNIST digits task. Ettore designed and performed the experiments for this work.</p>

<p><strong>Demos:</strong> Ettore, Alexander and Eyvind contributed to the tf.js demo.</p>

<p><strong>Writing and Diagrams:</strong> Ettore outlined the structure of the article, created graphs and videos, and contributed to the content throughout. Eyvind contributed to the content throughout, including video making and editing.</p>
<h3>Implementation details</h3>
<p><strong>TF.js playground.</strong> The demo shown in this work is made through Tensorflow.js (TF.js). In the colaboratory notebook described below, the reader can find customizable sizes of this playground, as well as more options for exploring pretrained models, trained without sampling from a state-preserving pool or mutation mechanisms, or with a CE loss.</p>

<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Furthermore, more training configurations are easily available: training without pooling, without mutations, with a different loss, with or without residual noise. In the colab, the user can find pretrained models for all these configurations, and customizable TF.js demos where one can try any configuration.</p>
<d-footnote-list></d-footnote-list>
<d-citation-list></d-citation-list>
<d-appendix>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
