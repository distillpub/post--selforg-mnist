<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style>
  </style>
  <script src="./tf.min.js"></script>
  <script type="module" src="./demo.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">{
      "title": "Self-Classifying MNIST",
      "description": "Training an end-to-end differentiable, self-organising cellular automata for classifying MNIST digits.",
      "authors": [
        {
          "author": "Ettore Randazzo",
          "authorURL": "https://oteret.github.io/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Alexander Mordvintsev",
          "authorURL": "https://znah.net/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Eyvind Niklasson",
          "authorURL": "https://eyvind.me/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Michael Levin",
          "authorURL": "http://www.drmichaellevin.org",
          "affiliation": "Allen Discovery Center at Tufts University",
          "affiliationURL": "http://allencenter.tufts.edu"
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <style>
    figure {
      text-align: center;
      margin-bottom: 0.5em;
      margin-top: 0.5em;
    }
    figure img {
      max-width: 100%;
      width: unset;
    }
    video {
      max-width: 100%;
    }
    .colab-root {
      display: inline-block;
      background: rgba(255, 255, 255, 0.75);
      padding: 4px 8px;
      border-radius: 4px;
      font-size: 11px!important;
      text-decoration: none;
      color: #aaa;
      border: none;
      font-weight: 300;
      border: solid 1px rgba(0, 0, 0, 0.08);
      border-bottom-color: rgba(0, 0, 0, 0.15);
      text-transform: uppercase;
      line-height: 16px;
    }

   span.colab-span {
      background-image: url(images/colab.svg);
      background-repeat: no-repeat;
      background-size: 20px;
      background-position-y: 2px;
      display: inline-block;
      padding-left: 24px;
      border-radius: 4px;
      text-decoration: none;
    }

    a.colab-root:hover{
      color: #666;
      background: white;
      border-color: rgba(0, 0, 0, 0.2);
    }

    /* TOC */
    @media(max-width: 1000px){
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    } 
    
    @media (min-width: 1000px){
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1180px){
      d-contents {
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }
    
    d-contents nav ul li {
      margin-bottom: .25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }


    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
        text-decoration: none;
    }

    /* code blocks to margins */
    @media (min-width: 1600px) {
      d-code {
        margin-top: -10px;
        grid-column-start: 12;
        grid-column-end: 14; 
      }
    }
    /* so title is on one line */
    d-title h1, d-title p {
      grid-column: middle;
    }
    
    d-article table th, d-article table td {
      text-align: center;
    }

    @media(max-width: 499px){
      d-article table th, d-article table td {
        font-size: 9px;
        padding: 0px 2px;
      }
    }

  </style>
  <script>
  // hack to edit font size in code snippets. guaranteed a better way to do 
  // this, but I'm not a webdev
  window.onload = function() {
    setTimeout(() => { document.querySelectorAll("d-code").forEach(function(e) {e.shadowRoot.querySelector('#code-container').style.fontSize = "0.7em"}); }, 3000);
  }
  </script>
  <d-title>
    <h1>Self-Classifying MNIST digits</h1>
    <p>Achieving Distributed Coordination with Neural Cellular Automata</p>


<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
    <symbol id="playIcon" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="pauseIcon" viewBox="0 0 24 24"><path d="M6 19h4V5H6v14zm8-14v14h4V5h-4z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="resetIcon" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"></path></symbol>
    <symbol id="colorwheel" viewBox="0 0 24 24"> 
  <path
     d="m 511.48146,205.29524 c 11.90567,44.43259 11.90567,84.97693 0,129.40952 L 487.31476,349.3424 270,270 490.20449,192.09986 z"
     id="path16"
     style="fill:#fefe33" />
  <path
     d="m 334.70476,28.518543 c 44.43259,11.905676 79.54502,32.17785 112.07193,64.704761 L 447.40047,118.09589 270,270 313.06484,40.508134 z"
     id="path20"
     style="fill:#fb9902" />
  <path
     d="m 446.77669,93.223304 c 32.52692,32.526916 52.79909,67.639346 64.70477,112.071936 L 270,270 z"
     id="path18"
     style="fill:#fabc02" />
  <path
     d="M 93.223305,93.223305 C 125.75022,60.696393 160.86265,40.42422 205.29524,28.518543 L 231.20546,44.501656 270,270 92.739568,120.0571 z"
     id="path28"
     style="fill:#fe2712" />
  <path
     d="m 205.29524,28.518543 c 44.43259,-11.905676 84.97693,-11.905676 129.40952,0 L 270,270 z"
     id="path22"
     style="fill:#fd5308" />
  <path
     d="m 28.518543,334.70476 c -11.905676,-44.43259 -11.905676,-84.97693 0,-129.40952 L 56.311276,186.62718 270,270 55.854788,349.40527 z"
     id="path26"
     style="fill:#8601af" />
  <path
     d="M 28.518543,205.29524 C 40.424219,160.86265 60.696393,125.75022 93.223305,93.223305 L 270,270 z"
     id="path30"
     style="fill:#a7194b" />
  <path
     d="M 205.29524,511.48146 C 160.86265,499.57578 125.75022,479.30361 93.223305,446.7767 L 95.307837,418.58874 270,270 231.0453,499.70648 z"
     id="path8"
     style="fill:#0247fe" />
  <path
     d="M 93.223305,446.7767 C 60.696393,414.24978 40.42422,379.13735 28.518543,334.70476 L 270,270 z"
     id="path24"
     style="fill:#3d01a4" />
  <path
     d="m 446.7767,446.7767 c -32.52692,32.52691 -67.63935,52.79908 -112.07194,64.70476 L 310.45335,496.38826 270,270 446.04632,421.15701 z"
     id="path12"
     style="fill:#66b032" />
  <path
     d="m 334.70476,511.48146 c -44.43259,11.90567 -84.97693,11.90567 -129.40952,0 L 270,270 z"
     id="path10"
     style="fill:#0391ce" />
  <path
     d="M 511.48146,334.70476 C 499.57578,379.13735 479.30361,414.24978 446.7767,446.7767 L 270,270 511.48146,334.70476 z"
     id="path14"
     style="fill:#d0ea2b" />
  <circle
     cx="270"
     cy="270"
     r="153.79581"
     id="circle32"
     style="fill:#ffffff" />
  </symbol> 
</svg>



<style>
#demo {
    font-size: 14px;
    user-select: none;
    grid-template-columns: auto;
    grid-template-rows: auto auto auto;
    grid-auto-flow: column;
    row-gap: 10px;
}

.hint a {
  color: inherit;
}

@media (min-width: 1000px) {
  #demo {
    grid-template-columns: 1fr 300px;
    grid-template-rows: auto auto;
  }
  #demo-controls {
    grid-row: 1/3;
  }
}

#demo-canvas {
    border: 1px solid lightgrey;
    image-rendering: pixelated;
    touch-action: none;
    width: 100%;
}

#demo-controls {
    line-height: 1em;
    display: grid;
    grid-template-columns: 120px auto;
    grid-template-rows: auto 70px 70px 70px 40px 1fr 1fr;
    row-gap: 20px;
    overflow: hidden;
}

@media (min-width: 1000px){
  #demo-controls {
    grid-template-rows: auto 70px 70px 70px 40px 1fr 1fr;
  }
}

#pattern-selector {
    grid-column: 1/3;
    display: grid;
    grid-template-columns: repeat(5, auto);
    justify-items: center;
}
@media (max-width: 1000px) and  (min-width: 500px) {
  #pattern-selector {
    grid-template-columns: repeat(10, auto);
  }
}

#pattern-selector * {
    width: 100%;
    /* background-image: url('images/emoji.png'); */
    cursor: pointer;
}
.icon {
    width: 30px; height: 30px;
    background: steelblue;
    fill: white;
    border-radius: 20px;
    padding: 5px;
    margin: 2px;
    cursor: pointer;
}
#model-selector {
    line-height: 1.4em;
}
#demo-tip{
    display: grid;
    grid-template-columns: 40px auto;
    align-items: center;
    column-gap: 10px;
    margin-bottom: 20px;
}
#pointer {
    width: 40px;
}
#status {
    font-size: 12px;
    color: rgba(0, 0, 0, 0.6);
    font-family: monospace;
}
#model-hints {
    color: rgba(0, 0, 0, 0.6);
    grid-column: 1/3;
}
#model-hints span {
    display: none;
}
.hint {
    color: rgba(0, 0, 0, 0.6);
    line-height: 1.4em;
    user-select: text;
}

input[type=range] {
  -webkit-appearance: none; /* Hides the slider so that custom slider can be made */
  width: 95%; /* Specific width is required for Firefox. */
  background: transparent; /* Otherwise white in Chrome */
  margin-bottom: 8px;
}

.hint a {
  font-size: 90%;
}

@media (max-width: 350px) {
  .hint a {
    font-size: 75%;
  }
}

input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
}

input[type=range]:focus {
  outline: none; /* Removes the blue border. You should probably do some kind of focus styling for accessibility reasons though. */
}

input[type=range]::-ms-track {
  width: 100%;
  cursor: pointer;

  /* Hides the slider so custom styles can be added */
  background: transparent;
  border-color: transparent;
  color: transparent;
}

/* Thumb */

/* Special styling for WebKit/Blink */
input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
  height: 16px;
  width: 16px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  margin-top: -7px; /* You need to specify a margin in Chrome, but in Firefox and IE it is automatic */
}

/* All the same stuff for Firefox */
input[type=range]::-moz-range-thumb {
  height: 16px;
  width: 16px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  border: none;
}

/* All the same stuff for IE */
input[type=range]::-ms-thumb {
  height: 16px;
  width: 16px;
  border-radius: 50%;
  background: grey;
  cursor: pointer;
}

/* Track */

input[type=range]::-webkit-slider-runnable-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]:focus::-webkit-slider-runnable-track {
  background: rgba(0, 0, 0, 0.15);
}

input[type=range]::-moz-range-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}
input[type=range]::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}

input[type="radio"] {
    background-color: steelblue;
}

#colab-hero-div { 
  grid-column: 1/3;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-top-width: 1px;
  border-top-style: solid;
  border-top-color: rgba(0, 0, 0, 0.1);
  padding-top: 15px;
}

#colab-hero {
  margin: auto;
  display: block;
  text-align: center;
  width: 200px;
  height: 16px;
}

#eraser {
  cursor: pointer;
  height: 50px;
  width: 50px;
  background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0), rgba(255, 255, 255, 1.0)), url(eraser.png);
  display: inline-block;
  background-repeat: no-repeat;
  background-size: contain;
}

#pencil {
  cursor: pointer;
  height: 50px;
  width: 50px;
  background-image: linear-gradient(to left, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0), rgba(255, 255, 255, 1.0)), url(pencil.png);
  display: inline-block;
  background-repeat: no-repeat;
  background-size: contain;
}

#paletteHint {
  grid-column: 1/3;
  color: rgba(0, 0, 0, 0.6);
  line-height: 1.4em;
  user-select: text;
  font-size: 75%;
}

#bin {
  width: 40px;
  display: block;
  margin: auto;
  cursor: pointer;
}

#bindiv {
  grid-column: 1/3;
}

.vidoverlay {
    position: absolute;
    width: 100%;
    height: 100%;
    background-position: center;
    background-image: url(images/play.svg);
    background-repeat: no-repeat;
    background-size: 15%;
    cursor: pointer;
    opacity: 0.8;
    z-index: 1;
    transition: opacity 1s;
}

.vc {
  position: relative;
}

</style>

<div class="l-body-outset grid" id="demo">
  
    <!-- fake canvas dimensions to ensure square in css dimensions -->  
    <canvas id="demo-canvas" width="512" height="512" class="color_heavy"></canvas>

    <div id="demo-tip">
            <img id="pointer" src="images/pointer.svg">
            <div class="hint">
                Click or tap and drag to draw shapes in the canvas. 
                Select different digits to load digits from the training set. Click again to get a different sample of the same digit. Press the bin to clear the canvas.
            </div>
    </div>

    <div id="demo-controls">
        <div id="pattern-selector" class="color_heavy">

        </div>
        <div style="text-align: center">
            <span id="play-pause">
                <svg class="icon" id="play"><use xlink:href="#playIcon"></use></svg>
                <svg class="icon" id="pause" style="display: none;"><use xlink:href="#pauseIcon"></use></svg>
            </span>
            <svg class="icon" id="reset"><use xlink:href="#resetIcon"></use></svg>
        </div>
        <div>
            Speed: <span id="speedLabel"></span><br>
            <input type="range" id="speed" min="-3" max="3" step="1" value="-2"><br>
            <div id="status">
               <!--  Step <span id="stepCount"></span> -->
                (<span id="ips"></span> step/s)
            </div>
        </div>
        <div id="eraser-pencil" style="text-align: center">
          <div id="eraser" style="filter: grayscale();"></div>
          <div id="pencil" ></div>
        </div>
        <div class="slidecontainer">
          Brush size:
          <input type="range" min="5" max="10" value="4" class="slider" id="brushSlider">
          <div id="status">(<span id='radius'>2.5</span> px)</div>
        </div>
        <div id="colorwheelicon" style="text-align: center">
          <img id="pointer" src="images/colorwheel.svg">
        </div>
        <div class="slidecontainer">
          Palette:
          <input type="range" min="0" max="360" value="0" class="slider" id="hueSlider">
          <div id="status">(<span id='hueValue'>0</span> deg)</div>
        </div>
        <div id="bindiv">
          <img id="bin" src="bin.png">
        </div>
        <div id="paletteHint">This article relies on using color to demonstrate classification label. If you have trouble distinguishing the colours of digits in the above legend, please try and adjust the slider above to see if there is an alternative colour palette for you. The chosen palette will propagate throughout the article.</div>
        <div id="colab-hero-div">
          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/mnist_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a>
        </div>
    </div>
</div>

<script type="module">
    import { mnistDemo } from './demo.js'
    tf.ENV.set('WEBGL_PACK', false);
    mnistDemo("demo", "demo-canvas");
</script>

</d-title>

<d-byline></d-byline>


<d-article>
<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#model">Model</a></div>
    <div><a href="#experiment-1">Experiments</a></div>
    <ul>
      <li><a href="#experiment-1">Self-classify, persist & mutate</a></li>
      <li><a href="#experiment-2">Solving the flickering regression</a></li>
    </ul>
    <div><a href="#robustness-and">Robustness and Limitations</a></div>
    <div><a href="#related-work">Related Work</a></div>
    <div><a href="#discussion">Discussion</a></div>
  </nav>
</d-contents>
<p>Growing Neural Cellular Automata <d-cite key="mordvintsev2020growing"></d-cite> demonstrated how simple cellular automata (CAs) can learn to self-organise into complex shapes while being resistant to perturbations. Such a computational model approximates a solution to an open question in biology, namely, how do cells cooperate to create a complex multicellular anatomy and work to regenerate it upon damage? The model parameterizing the cells’ rules is parameter-efficient, end-to-end differentiable, and illustrates a new approach to modeling the regulation of anatomical homeostasis. In this work we use this model to show how CAs can be applied to a common task in machine learning: classification. We pose the question: <i>can CAs use local message passing to achieve global agreement on what digit they compose?</i></p>
<p>Our question is closely related to another unsolved problem in cell and developmental biology: anatomical surveillance and the ability to decide whether a pattern is correct or whether cells need to remodel the current anatomy. For example, a salamander tail surgically transplanted to the flank slowly remodels to a limb - the organ that belongs at this location <d-cite key="farinella-ferruzza_1956"></d-cite>. Similarly, tadpoles with craniofacial organs in the wrong positions become normal frogs because they remodel their face to place the eye, mouth, nostrils, etc. in their correct locations <d-cite key="vandenberg_adams_levin_2012"></d-cite>. All of these examples illustrate the ability of biological systems to determine their current anatomical structure and decide whether it matches a species-specific target morphology <d-cite key="pezzulo_levin_2016"></d-cite>. Despite the recent progress in molecular biology of genes required for this process, it is now essential to develop a computational understanding of candidate algorithms sufficient for cell collectives to measure and classify their own large-scale morphology. More broadly, it is important to create computational models of swarm intelligence that make explicit and distinguish the dynamics of basal cognition of single cells vs. cell collectives <d-cite key="Baluska_Levin_2016"></d-cite><d-cite key="Lyon_2006"></d-cite>. </p>
<h3>Self-classifying MNIST task</h3>
<p>Suppose a population of agents are arranged in a grid. They do not know where they are in the grid, and they can only communicate with their immediate neighbors. They can also observe whether a neighbor is missing. Now suppose these agents are arranged to form a large digit. Given that all the agents operate under the same rules, can they form a communication protocol such that, after a number of iterations of communication, <i>all of the agents know which digit they are forming</i><i>?</i></p>
<p>Furthermore, if some agents were to be removed and added to form a new digit from a preexisting one, would they be able to know which the new digit is?</p>

<p>Because digits are not rotationally invariant (i.e. 6 is a rotation of 9), we presume the agents must be made aware of their orientation with respect to the grid. Therefore, while they do not know where they are, they know where up, down, left and right are. The biological analogy here is a situation where the remodeling structures exist in the context of a larger body and a set of morphogen gradients or tissue polarity that indicate directional information with respect to the three major body axes.</p>

<p>We introduce the self-classifying MNIST task.</p>

<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:500px;">
  <object data="figures/mnist_digits.png" type="image/png" style="width:100%"></object>
<figcaption style="">
A visualisation of a random sample of digits from MNIST, each shaded by the colour corresponding its label.
</figcaption>
</figure></p>

<p>Each sample of the MNIST dataset <d-cite key="lecun_mnist"></d-cite> consists of a 28x28 image with a single monochrome channel that is classically displayed in greyscale. The label is an integer in $[0,9]$.</p>

<p>Our goal is for all cells that make up the digit to correctly output the label of the digit. To convey this structural information to the cells, we make a distinction between alive and dead cells by rescaling the values of the image to [0, 1]. Then we treat a cell as alive if its value in the MNIST sample is larger than 0.1. The intuition here is that we are placing living cells in a cookie cutter and asking them to identify the global shape of the cookie cutter. We visualize the label output by assigning a color to each cell, as you can see above. We use the same mapping between colors and labels throughout the article. Please note that there is a slider in the interactive demo controls which you can use to adjust the color palette if you have trouble differentiating between the default colors. </p>
<h2 id='model'>Model</h2>
<p>In this article, we use a variant of the neural cellular automata model described in Growing Cellular Automata <d-cite key="mordvintsev2020growing"></d-cite>. We refer readers unfamiliar with its implementation, to the original <a href="https://distill.pub/2020/growing-ca/#model">”Model”</a> section. Here we will describe a few areas where our model diverges from the original.</p>
<h3>Target labels</h3>
<p>The work in Growing CA used RGB images as targets, and optimized the first three state channels to approximate those images. For our experiments, we treat the last ten channels of our cells as a pseudo-distribution over each possible label (digit). During inference, we simply pick the label corresponding to the channel with the highest output value.</p>
<h3>Alive cells and cell states</h3>
<p>In Growing CA we assigned the cell's state to be “dead” or “alive” based on the strength of its alpha channel and the activity of its neighbors, in a manner similar to the rules of the Game of Life. In details for the Growing CA model, “alive” cells are cells which update their state and dead cells are “frozen” and do not undergo updates. In contrast to biological life, what we call “dead” cells aren’t dead in the sense of being non-existent or decayed, but rather frozen: they are visible to their neighbors and maintain their state throughout the simulation. In this work, meanwhile, we use input pixel values to determine whether cells are alive or dead and perform computations with alive cells only<d-footnote> As introduced in the previous section, cells are considered alive if their normalized grey value is larger than 0.1.</d-footnote>. It is important to note that the values of MNIST pixels are exposed to the cell update rule as an immutable channel of the cell state. In other words, we make cells aware of their own pixel intensities as well as those of their neighbors. Given 19 mutable cell state channels (nine general purpose state channels for communication and ten output state channels for digit classification) and an immutable pixel channel, each cell perceives 19 + 1 state channels and only outputs state updates for the 19 mutable state channels.</p>

<p><strong>A note on digit topology. </strong>Keen readers may notice that our model requires each digit to be a single connected component in order for classification to be possible, since any disconnected components will be unable to propagate information between themselves. We made this design decision in order to stay true to our core biological analogy, which involves a group of cells that is trying to identify its global shape. The vast majority of samples from MNIST are fully connected. But some aren’t. We do not expect our models to classify non-connected minor components correctly, but we do not remove them<d-footnote> This choice complicates comparison between the MNIST train/test accuracies of neural network classifiers vs. CAs. However, such a comparison is not in scope of this article.</d-footnote>.</p>
<h3>Perception</h3>
<p>The Growing CA article made use of fixed 3x3 convolutions with Sobel filters to estimate the gradient of state in x and y. We found that fully trainable 3x3 kernels outperformed their fixed counterparts and so used trainable kernels in this work.</p>

<p><strong>A note on model size. </strong>Like the Growing CA model, our MNIST CA is small by the standards of deep learning - it has less than 25k parameters. In this work, we are interested in demonstrating a novel approach to classification. We do not attempt to maximise the validation accuracy of the model by increasing the number of parameters or any other tuning. We suspect that, as with other deep neural network models, one would observe a positive correlation between accuracy and model size.</p>
<h2 id='experiment-1'>Experiment 1: Self-classify, persist and mutate</h2>
<p>For our first experiment, we use the same training paradigm as was discussed in Growing CA. We train with a pool of initial samples to allow the model to learn to persist and then perturb the converged states. However, our perturbation is different. Previously, we destroyed the states of cells at random in order to make the CAs resistant to destructive perturbations (analogous to traumatic tissue loss).In this context, perturbation has a slightly different role to play. Here we aim to build a CA model that not only has regenerative properties, but also <i>has the ability to correct itself when the shape of the overall digit changes. </i>Biologically, this corresponds to a teratogenic influence during development or the case of an incorrect or incomplete remodeling event such as metamorphosis or rescaling. The distinction between training our model from scratch and training it to accommodate perturbations is subtle but important. An important feature of life is the ability to react to changing environmental conditions and external forces. If our virtual cells simply learned to recognize a digit and then entered some dormant state and did not react to any further changes, we would be missing this key property of living organisms. One could imagine a trivial solution in the absence of perturbations, where a single wave of information is passed from the boundaries of the digit inwards then back out, in such a way that all cells could agree on a correct classification. By introducing perturbations to new digits, the cells have to be in constant communication and achieve a “dynamic homeostasis” - continually “kept on their toes” for any new or further communication from their neighbours.</p>

<p>We randomly mutate the underlying digit in order to build robustness to perturbations. Starting from a certain digit and after some time evolution, we sample a new digit, erase all cell states that are not present in both digits and bring alive the cells that were not present in the original digit but are present in the new digit. This kind of mutation teaches CAs to learn to process new information and adapt to changing conditions. It also exposes the cells to training states where all of the cells that remain after a perturbation are misclassifying the new digit and must recover from this catastrophic mutation. This in turn forces our CAs to learn to change their own classifications to adapt to changing global structure.</p>

<p>We use a pixel-wise (cell-wise) cross entropy loss on the last ten channels of each pixel, applying it after letting the CA evolve for 20 steps.</p>
<p><figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="figures/ce_runs.mp4#t=0.1" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
A first attempt at having the neural CAs classify digits. Each digit is a separate evolution of the neural CA, with the visualisations collated. Halfway through, the underlying digit is swapped for a new one - a "mutation".</figcaption>
</figure></p>

<p>The video above shows the CA classifying a batch of digits for 200 steps. We then mutate the digits and let the system evolve and classify for a further 200 more steps.</p>

<p>The results look promising overall and we can see how our CAs are able to recover from mutations. However, keen readers may notice there is often a lack of total agreement between the cells. Often, the majority of the digit is classified correctly, but some outlier cells are still convinced they are part of a different digit, often switching back and forth in an oscillating pattern, creating a flickering effect in the visualization. This is not ideal, since we would like the population of cells to reach stable, total agreement. The next experiment troubleshoots this undesired behaviour.</p>
<h2 id='experiment-2'>Experiment 2: Solving the flickery regression</h2>
<p>Quantifying a qualitative issue is the first step to solving it. We propose a metric to track <strong>average cell accuracy</strong>: defined as the mean percentage of cells that have a correct output. We track this metric both before and after mutation.<figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width: 700px">
  <object data="figures/ce_accuracy.png" type="image/png" style="width:100%"></object>
<figcaption style="">
Average accuracy across the cells in a digit over time.</figcaption>
</figure></p>
<p>In the figure above, we show the mean percentage of correctly classified cells, across the test set, over 400 steps. At step 200, we randomly mutate the digit. Accordingly, we see a brief drop in accuracy as the cells re-organise and eventually come to agreement on what the new digit may be.</p>

<p>We immediately notice an interesting phenomenon: the cell accuracy appears to decrease over time, after the cells have come to an agreement. However, the graph does not necessarily reflect the qualitative issue of unstable labels that we set out to solve. The slow decay in accuracy may be a reflection of the lack of total agreement, but doesn’t capture the stark instability issue.</p>

<p>Instead of looking at the mean agreement perhaps we should measure <strong>total agreement</strong>. We define total agreement as the percentage of samples from a given batch wherein all the cells output the same label. </p>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/ce_agreement.png" type="image/png" style="width:100%"></object>
<figcaption style="">
Average total agreement among cells across the test set in MNIST, over time.</figcaption>
</figure></p>

<p>This metric does a better job of capturing the issues we are seeing. The total agreement starts at zero percent and then spikes up to roughly 78%, only to lose more than 10% agreement over the next 100 steps. Again, there does not appear to be significantly different behaviour after mutation. Our model is not only unstable in the short term, exhibiting flickering, but is also unstable over longer timescales. As time goes on, cells are becoming less sure of themselves. Let’s inspect the inner states of the CA to see if we gain insight.</p>

<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/ce_magnitude.png" type="image/png" style="width:100%"></object>
<figcaption style="">
Average magnitude of the state channels and residual updates in active cells over time in the test set.</figcaption>
</figure></p>
<p>The figure above shows the evolution, over time, of the average magnitude of the state values of active cells (solid line), and the average magnitude of the residual updates for the active cells firing at each timestamp (dotted line). We can observe two important properties of our model: 1) the average magnitude of each cell’s internal states is growing monotonically (at least in the time-scales in our experiments); 2) the average magnitude of the residual updates does not change significantly over time. We theorize that a successful CA model should stabilize the magnitude of its internal states once cells find agreement.</p>

<p><strong>Using an</strong><strong> $L_2$</strong><strong> loss</strong><strong>:</strong><strong> </strong>One problem with cross entropy loss is that it tends to push raw logit values higher indefinitely. Meanwhile, two sets of logits can have vastly different values, but essentially the same prediction over classes. As such, training the CA with cross-entropy loss neither requires nor encourages a shared reference range for logit values inside one cell and between different cells, making it difficult for the cells to effectively communicate and stabilize. Finally, we theorize that large magnitudes in the classification channels may in turn lead the remaining (non-classification) state channels to adapt to a high magnitude regime. More specifically, we believe that cross-entropy loss pushes classification logits higher, which depend on the recurrent residual update, which depend on the incoming state channel values, leading to these other state channel values being pushed higher as well. With these problems in mind, we instead try training our model with a pixel-wise $L_2$ loss using one-hot vectors as targets. Intuitively, this solution should be more stable since the raw state channels for classification are encouraged to stay close to the range $[0, 1]$ and a properly classified digit in a cell will have exactly one classification channel set to 1 and the rest to 0. An $L_2$ loss should decrease the magnitude of all the internal state channels while allowing a shared reference range for the classification channels. </p>

<p><strong>Adding noise to the residual</strong>: A key idea in most regularization schemes is to inject noise to make a classifier or model more robust.  We introduce noise sampled from a normal distribution with zero mean and 0.02 standard-deviation to the residual updates. The noise is added before the random mask of updates.</p>
<p><figure>
<div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="figures/l2_runs.mp4#t=0.1" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
<figcaption style="">
Neural CA trained with $L_2$ loss, exhibiting less instability after converging to a label.</figcaption>
</figure></p>
<p>The video above shows a batch of runs with the augmentations in place. Qualitatively, the result looks much better as there is less flickering and more total agreement. We now compare it to the original method using the metrics we defined.</p>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/ce_vs_l2_metrics.png" type="image/png" style="width:100%"></object>
<figcaption style="">
Comparison of average accuracy and total agreement 
 when using cross-entropy and when using $L_2$ loss.</figcaption>
</figure></p>

<div id=accTable style=overflow-x:scroll;grid-column:page;max-width:700px;margin-left:auto;margin-right:auto;><table class=model_table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Top accuracy</th>
<th align="center">Accuracy @ 200 </th>
<th align="center">Top agreement</th>
<th align="center">Agreement @ 200</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">CE</td>
<td align="center"><b>96.2 @ 80</b></td>
<td align="center"><b>95.3</b></td>
<td align="center">77.9 @ 80</td>
<td align="center">66.2</td>
</tr>
<tr>
<td align="center">$L_2$</td>
<td align="center">95.0 @ 95</td>
<td align="center">94.7</td>
<td align="center">85.5 @ 175</td>
<td align="center">85.2</td>
</tr>
<tr>
<td align="center">$L_2$ + Noise</td>
<td align="center">95.4 @ 65</td>
<td align="center"><b>95.3</b></td>
<td align="center"><b>88.2 @ 190</b></td>
<td align="center"><b>88.1</b></td>
</tr>
</tbody>
</table>
</div>


<p>The figure and table above show that cross-entropy achieves the highest accuracy of all models at roughly 80 steps. However, the accuracy at 200 steps is the same as the $L_2$ + Noise model. While accuracy and agreement degrade over time for all models, it is evident that the $L_2$ + Noise is the most stable configuration. Moreover, the total agreement after 200 steps of $L_2$ + Noise is 88%, an improvement of more than 20% as compared to the cross-entropy model. </p>
<h3>Internal states</h3>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:700px">
  <object data="figures/magnitude_comparison.png" type="image/png" style="width:100%"></object>
<figcaption style="">
Average magnitude of state channels over time for $L_2$ loss and cross-entropy loss.</figcaption>
</figure></p>
<p>Let’s compare the internal states of the augmented model versus the original. The figure above shows how switching to an $L_2$ loss successfully stabilizes the magnitude of the states, and how residual updates quickly converge to small values after an agreement is about to be reached.</p>

<p><figure>
<div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted preload="auto" width:100%>
      <source src="figures/l2n_horiz_states.mp4#t=0.5" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
<figcaption style="">
Visualisation of internal state channel values during mutations. Note the accelerated timeline after a few seconds showing the relative stability of the channel values.</figcaption>
</figure></p>
<p>Here are the resulting dynamics of the internal states for the final model. For visualization purposes, we have squashed the internal state values using arctan, as the majority of state values have a magnitude of less than one but a small number are much larger in magnitude. The states converge to stable configurations quickly and the state channels exhibit spatial continuity with the neighbouring states. More specifically, we don’t see any stark discontinuities in state values between neighbouring pixels. Applying a mutation causes the CA to readapt to the new shape and form a new classification in just a few steps, after which its internal values are stable.</p>
<h2 id='robustness-and'>Robustness and Limitations</h2>
<p>Random digit mutations were used during training to ensure the resulting CA was responsive to external changes. Biologically, this helps understand the insensitivity of some large-scale anatomical control mechanisms to mutations. For example, planaria continuously accumulate mutations over millennia of somatic inheritance but still always regenerate the correct morphology in nature (and exhibit no genetic strains with new morphologies) <d-cite key="LEVIN2019125"></d-cite>. </p>
<p>This robustness to change is of critical importance for making an interactive model - in order for the cells to classify drawings “live” (an eight being completed from a six, for instance, should quickly re-classify itself as an eight). We encourage the readers to play with the interactive demo, and experience this for themselves. In this section, we want to showcase a few behaviours we found interesting.</p>

<p><figure>
    <div class="vc">
<div class="vidoverlay"></div>    
<video playsinline muted width="320px" preload="auto">
      <source src="figures/drawing_mutations.mp4#t=9.2" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
<div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
<figcaption style="">
Demonstration of the CA successfully re-classifying a digit when it is modified by hand.</figcaption>
</figure></p>

<p>The video above shows how the CA is able to interactively adjust to our own writing, and to change classification when the drawn digit is adjusted.</p>
<h3>Robustness to out-of-training configurations</h3>
<p>In the field of machine learning, researchers take great interest in how their models behave with out-of-distribution data. In the experiments sections of this article, we evaluated our model on the test set of MNIST. In this section, we go further and see how the model reacts to digits drawn by us and not sampled from MNIST at all. We vary the shapes of the digits until the model is no longer capable of classifying them correctly. Every classification model inherently contains certain inductive biases that render them more or less robust to generalizing to out-of-band data. Our model can be seen as a recurrent convolutional model and thus we expect it to exhibit some of the key properties of traditional convolutional models such as translation invariance. Moreover, we strongly believe the self-organising nature of this model introduces a novel inductive bias which may have interesting properties of its own. Biology offers examples of repairing to novel configurations: 2-headed planaria, once created, regenerate to this new configuration <d-cite key="OVIEDO2010188"></d-cite> which was not present in the evolutionary “training set”. </p>
<p><figure>
<div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="320px" preload="auto">
      <source src="figures/drawing_bad.mp4#t=9.0" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>   
<div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
<figcaption style="">
Demonstration of some of the failure cases of the CA.</figcaption>
</figure></p>

<p>Above, we can see our CA failing to classify some variants of 1 and 9. This is likely because MNIST training data is not sufficiently representative of all writing styles. We hypothesize that more varied and extensive datasets would improve performance. The model often oscillates between two attractors (of competing digit labels) in these situations. This property could not arise from static classifiers such as traditional convolutional neural networks.</p>

<p><figure>
<div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="320px" preload="auto">
      <source src="figures/mnist_ablation.mp4#t=16.0" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
<div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
<figcaption style="">
Demonstration of the inherent robustness of the model to unseen sizes and variants of numbers.</figcaption>
</figure></p>
<p>By construction, our CA is translation invariant. More surprisingly, we noticed that our model is also scale-invariant for out-of-distribution digit sizes up to a certain point. Alas, it does not generalize well enough to work for arbitrary lengths and widths.</p>
<p><figure>
<div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="320px" preload="auto">
      <source src="figures/mnist_chimeras.mp4#t=16.0" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</div>
<div>
  <object data="figures/horiz_legend.jpeg" type="image/png" style="width:320px"></object>
   </div>
<figcaption style="">
Demonstration of the behaviour of the model with chimeric configurations.</figcaption>
</figure></p>
<p>It is also interesting to see how our CA classifies “chimeric digits”: shapes composed of multiple digits. First, when creating a 3 and 5 chimera, it appears that the classification of 3 dominates the other. Second, when creating a 8 and 9 chimera, the CAs reach an oscillating attractor where parts of 8 and 9 are correctly classified. Third, when creating a 6 and 9 chimera, the CAs still converge to an oscillating attractor, but the “6” is misclassified as 4. These types of phenomena are important in biology to begin to develop predictive models for morphogenetic outcome of chimeric cell collectives. We still do not have a framework for knowing in advance what anatomical structures will form from a combination of, for example leg+tail blastema cells in an axolotl, heads of planaria housing stem cells from species with different head shapes, or composite embryos consisting of for example frog and axolotl blastomeres <d-cite key="mustard2014, suchy2018"></d-cite>. Likewise, the design of information signals that induce the emergence of desired tissue patterns from a chimeric cellular collective, in vitro or in vivo, remains an open problem.</p>
<h2 id='related-work'>Related work</h2>
<p>This article is follow-up work to Growing Neural Cellular Automata <d-cite key="mordvintsev2020growing"></d-cite>, and it is meant to be read after the latter. In this article, we purposefully skim over details of the original model and we refer the reader to the Growing Neural Cellular Automata article for the full model description and related work.</p>

<p><strong>MNIST and CA</strong></p>
<p>Since CAs are easily applied to two dimensional grids, many researchers wondered if they could use them to somehow classify the MNIST dataset <d-cite key="lecun_mnist"></d-cite>. We are aware of work that combines CAs with Reservoir Computing <d-cite key="alej2018reservoir, alej2020reservoir"></d-cite>, Boltzmann Machines <d-cite key="matsubara2018"></d-cite>, Evolutionary Strategies <d-cite key="oliveira2008"></d-cite>, and ensemble methods <d-cite key="WALI201877, jastrzebska2017"></d-cite>. To the best of our knowledge, we are the first to train end-to-end differentiable Neural CAs for classification purposes, and we are the first to introduce the self-classifying variant of MNIST wherein each pixel in the digit needs to coordinate locally in order to come to a global agreement about its label.</p>
<h2 id='discussion'>Discussion</h2>
<p>This article serves as a proof of concept for how simple self-organising systems such as CA can be used for classification if trained end-to-end through backpropagation.</p>

<p>Our model adapts to writing and erasing and is surprisingly robust to certain ranges of digit stretching and brush widths. We hypothesize that self-organising models with constrained capacity may be inherently robust and have good generalisation properties. We encourage future work to test this hypothesis.</p>

<p>From a biological perspective, our work shows we can teach things to a collective of cells that they could not learn individually (by training or engineering a single cell). Training cells in unison (while communicating with each other) allows them to learn more complex behaviour than any attempt to train them one by one, which has important implications for strategies in regenerative medicine. The current focus on editing individual cells at the genetic or molecular signaling level faces fundamental barriers when trying to induce desired complex, system-level outcomes (such as regenerating or remodeling whole organs). The inverse problem of determining which cell-level rules (e.g., genetic information) must be changed to achieve a global outcome is very difficult. In contrast and complement to this approach, we show the first component of a roadmap toward developing effective strategies for communication with cellular collectives. Future advances in this field may be able to induce desired outcomes by using stimuli at the system’s input layer (experience), not hardware rewiring, to re-specify outcomes at the tissue, organ, or whole-body level <d-cite key="Mathews_Levin_2018"></d-cite><d-cite key="Pezzulo_Levin_2015"></d-cite>.</p>

</d-article>
<d-appendix>
<h3>Acknowledgments</h3>
<p>We thank Sam Greydanus for his thoughtful and extensive proofreading and conversations. We thank Zhitao Gong, Alex Groznykh, Nick Moran, Peter Whidden for their valuable conversations and feedback.</p>
<h3>Author Contributions</h3>
<p><strong>Research:</strong> Alexander came up with the Self-Organising Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Alexander came up with the self-classifying MNIST digits task. Ettore designed and performed the experiments for this work. </p>

<p><strong>Demos:</strong> Ettore, Eyvind and Alexander contributed to the demo.</p>

<p><strong>Writing and Diagrams:</strong> Ettore outlined the structure of the article, created graphs and videos, and contributed to the content throughout. Eyvind contributed to the content throughout, including video making and substantive editing and writing. Michael made extensive contributions to the article text, providing the biological context for this work.</p>
<h3>Implementation details</h3>
<p><strong>TF.js playground.</strong> The demo shown in this work is made through Tensorflow.js (TF.js). In the colaboratory notebook described below, the reader can find customizable sizes of this playground, as well as more options for exploring pretrained models, trained without sampling from a pool of different initial states, or mutation mechanisms, or using a cross-entropy loss.</p>

<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Furthermore, more training configurations are easily available: training without pooling, without mutations, with a different loss, with or without residual noise. In the colab, the user can find pretrained models for all these configurations, and customizable TF.js demos where one can try any configuration.</p>
<d-footnote-list></d-footnote-list>
<d-citation-list></d-citation-list>
<d-appendix>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
